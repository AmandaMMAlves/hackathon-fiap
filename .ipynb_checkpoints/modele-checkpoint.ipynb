{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "from distutils.file_util import copy_file\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# %%Obtendo o dataset\n",
        "\n",
        "caminho_pasta = \"data/\"\n",
        "classes = ['cortantes', 'nao_cortantes']\n",
        "\n",
        "pasta_imgs_cortantes = Path(caminho_pasta + classes[0])\n",
        "pasta_imgs_nao_cortantes = Path(caminho_pasta + classes[1])\n",
        "\n",
        "extensao = \".jpg\"\n",
        "\n",
        "#Vou renomear todos os itens para que todas as imagens sejam .jpg\n",
        "def renomear_arquivos(pasta_imgs, extensao):\n",
        "    for arquivo in pasta_imgs.iterdir():\n",
        "        if arquivo.is_file():\n",
        "            novo_nome = arquivo.with_suffix(extensao)\n",
        "            arquivo.rename(novo_nome)\n",
        "\n",
        "    print(f\"Arquivos na pasta {pasta_imgs} foram renomeados para a extens\u00e3o {extensao}.\\n\\n\")\n",
        "\n",
        "renomear_arquivos(pasta_imgs_cortantes, extensao)\n",
        "renomear_arquivos(pasta_imgs_nao_cortantes, extensao)\n",
        "\n",
        "\n",
        "filepaths_cortantes = list(pasta_imgs_cortantes.glob(r'**/*.jpg'))\n",
        "filepaths_nao_cortantes = list(pasta_imgs_nao_cortantes.glob(r'**/*.jpg'))\n",
        "\n",
        "def exibir_numero_itens_filepath(filepaths_cortantes, filepaths_nao_cortantes):\n",
        "    print(\"Pasta de objetos cortantes:\")\n",
        "    print(len(filepaths_cortantes))\n",
        "    print(\"Pasta de objetos n\u00e3o cortantes:\")\n",
        "    print(f\"{len(filepaths_nao_cortantes)} \\n\\n\")\n",
        "\n",
        "exibir_numero_itens_filepath(filepaths_cortantes, filepaths_nao_cortantes)\n",
        "\n",
        "print(\"Realizando balanceamento manual...\\n\\n\")\n",
        "\n",
        "# %%Balanceando manualmente o dataset\n",
        "#Vou pegar a quantidade de imagens da pasta com menos imagens e vou pegar a mesma quantidade da outra pasta\n",
        "def manual_balance(filepaths_cortantes, filepaths_nao_cortantes):\n",
        "    minimo = min(len(filepaths_cortantes), len(filepaths_nao_cortantes))\n",
        "    filepaths_cortantes = filepaths_cortantes[:minimo]\n",
        "    filepaths_nao_cortantes = filepaths_nao_cortantes[:minimo]\n",
        "    return filepaths_cortantes, filepaths_nao_cortantes\n",
        "\n",
        "filepaths_cortantes, filepaths_nao_cortantes = manual_balance(filepaths_cortantes, filepaths_nao_cortantes)\n",
        "\n",
        "exibir_numero_itens_filepath(filepaths_cortantes, filepaths_nao_cortantes)\n",
        "\n",
        "def get_labels_images(filepaths):\n",
        "    labels = []\n",
        "    images = []\n",
        "\n",
        "    image_size = 64\n",
        "\n",
        "    for filepath in filepaths:\n",
        "        head = os.path.split(filepath)\n",
        "        obj = os.path.split(head[0])\n",
        "\n",
        "        labels.append(obj[1])#armazena rotulo cortante ou nao_cortante obtido pelo path\n",
        "\n",
        "        img = cv2.imread(str(filepath))\n",
        "\n",
        "        # Check if image was loaded correctly\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (image_size, image_size)).astype('float32') / 255.0 #redimensiona e normaliza imagem\n",
        "            images.append(img)\n",
        "        else:\n",
        "            print(f\"Failed to load image: {filepath}\") # Print the problematic filepath\n",
        "\n",
        "    #Converte a imagem em lista de array\n",
        "    images = np.array(images)\n",
        "    #Converte as labels para uma lista de array\n",
        "    labels = np.array(labels)\n",
        "    \n",
        "    return labels, images\n",
        "\n",
        "print(\"Obtendo r\u00f3tulo das imagens...\\n\\n\")\n",
        "labels_cortantes, images_cortantes = get_labels_images(filepaths_cortantes)\n",
        "labels_nao_cortantes, images_nao_cortantes = get_labels_images(filepaths_nao_cortantes)\n",
        "labels = np.concatenate((labels_cortantes, labels_nao_cortantes))\n",
        "images = np.concatenate((images_cortantes, images_nao_cortantes))\n",
        "\n",
        "print(\"Quantidade de imagens e r\u00f3tulos obtidos:\")\n",
        "print(len(images))\n",
        "print(len(labels))\n",
        "\n",
        "print(\"Imagens e r\u00f3tulos obtidos com sucesso!\\n\\n\")\n",
        "\n",
        "# %%Construindo o dataframe\n",
        "print(\"Iniciando a constru\u00e7\u00e3o do dataframe... \\n\\n\")\n",
        "filepaths = filepaths_cortantes + filepaths_nao_cortantes\n",
        "pd_filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "pd_labels = pd.Series(labels, name='Label')\n",
        "\n",
        "df = pd.concat([pd_filepaths, pd_labels], axis=1)\n",
        "print(df['Label'].value_counts())\n",
        "\n",
        "# %%Embaralhando o dataset\n",
        "print(\"Embaralhando o dataset e exibindo as primeiras linhas... \\n\\n\")\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "print(df.head())\n",
        "print(df['Label'].value_counts())\n",
        "print(\"Dataset embaralhado com sucesso!\\n\\n\")\n",
        "\n",
        "# %%Separando a base de treino e teste\n",
        "print(\"Separando base de 80% treino e de 20% teste... \\n\\n\")\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "\n",
        "print(\"Base de treino e teste separadas com sucesso!\\n\\n\")\n",
        "\n",
        "# %%Preparando os dados para a rede convolucional\n",
        "print(\"Preparando dados para a rede convolucional... \\n\\n\")\n",
        "#Achatando os dados de teste e treino\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2] * x_train.shape[3])\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2] * x_test.shape[3])\n",
        "\n",
        "#Usar o labelEncoder para transformar nossas labels em numeros binarios\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "#Converte um vetor de classe (inteiros) em uma matriz de classe categorica\n",
        "y_train_tf = keras.utils.to_categorical(y_train_encoded, len(classes))\n",
        "y_test_tf = keras.utils.to_categorical(y_test_encoded, len(classes))\n",
        "\n",
        "\"\"\"Vamos configurar agora como pr\u00f3ximo passo o ModelCheckpoint para usar os melhores pesos \n",
        "para este modelo. Um dos benef\u00edcios do ModelCheckpoint \u00e9 salvar uma c\u00f3pia do modelo em disco \n",
        "em intervalos regulares (como ap\u00f3s cada \u00e9poca de processamento) para que voc\u00ea possa retomar \n",
        "o treinamento a partir do ponto em que parou, minimizando perdas de tempo e recursos computacionais\"\"\"\n",
        "#Salva os melhores pesos para esse modelo\n",
        "checkpointer = ModelCheckpoint(filepath='weights.best.hdf5.keras', verbose=0, save_best_only=True)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}